{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import joblib\n",
        "\n",
        "class LinearRegressionScratch:\n",
        "    def __init__(self, learning_rate=0.01, epochs=500):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.W = None\n",
        "        self.b = None\n",
        "        self.scaler_X = StandardScaler()\n",
        "        self.scaler_y = StandardScaler()\n",
        "\n",
        "    def _mse_loss(self, y_pred, y_true):\n",
        "        return torch.mean((y_pred - y_true) ** 2)\n",
        "\n",
        "    def fit(self, csv_file):\n",
        "        # Load data\n",
        "        df = pd.read_csv(csv_file)\n",
        "        X = df.iloc[:, :-1].values\n",
        "        y = df.iloc[:, -1].values.reshape(-1, 1)\n",
        "\n",
        "        # Scale\n",
        "        X_scaled = self.scaler_X.fit_transform(X)\n",
        "        y_scaled = self.scaler_y.fit_transform(y)\n",
        "\n",
        "        # Train/test split\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X_scaled, y_scaled, test_size=0.2, random_state=42\n",
        "        )\n",
        "\n",
        "        # Convert to torch tensors\n",
        "        X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "        y_train = torch.tensor(y_train, dtype=torch.float32)\n",
        "\n",
        "        num_features = X_train.shape[1]\n",
        "\n",
        "        # Initialize weights and bias\n",
        "        self.W = torch.randn((num_features, 1), requires_grad=True)\n",
        "        self.b = torch.zeros((1,), requires_grad=True)\n",
        "\n",
        "        # Training loop\n",
        "        for epoch in range(self.epochs):\n",
        "            y_pred = X_train @ self.W + self.b\n",
        "            loss = self._mse_loss(y_pred, y_train)\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                self.W -= self.learning_rate * self.W.grad\n",
        "                self.b -= self.learning_rate * self.b.grad\n",
        "\n",
        "            self.W.grad.zero_()\n",
        "            self.b.grad.zero_()\n",
        "\n",
        "            if (epoch + 1) % 50 == 0:\n",
        "                print(f\"Epoch {epoch+1}: Loss = {loss.item():.4f}\")\n",
        "\n",
        "        # Save scalers & params\n",
        "        joblib.dump(self.scaler_X, \"scaler_X.pkl\")\n",
        "        joblib.dump(self.scaler_y, \"scaler_y.pkl\")\n",
        "        torch.save({\"W\": self.W, \"b\": self.b}, \"linear_params.pt\")\n",
        "\n",
        "    def predict(self, X_new):\n",
        "        if self.W is None or self.b is None:\n",
        "            # Load saved model if available\n",
        "            checkpoint = torch.load(\"linear_params.pt\")\n",
        "            self.W = checkpoint[\"W\"]\n",
        "            self.b = checkpoint[\"b\"]\n",
        "            self.scaler_X = joblib.load(\"scaler_X.pkl\")\n",
        "            self.scaler_y = joblib.load(\"scaler_y.pkl\")\n",
        "\n",
        "        # Scale input\n",
        "        X_scaled = self.scaler_X.transform(np.array(X_new).reshape(1, -1))\n",
        "        X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
        "\n",
        "        # Prediction\n",
        "        y_pred_scaled = X_tensor @ self.W + self.b\n",
        "        y_pred = self.scaler_y.inverse_transform(y_pred_scaled.detach().numpy())\n",
        "\n",
        "        return y_pred[0][0]\n"
      ],
      "metadata": {
        "id": "9vFSgY3jSOel"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create fake data\n",
        "num_samples = 100\n",
        "num_features = 5\n",
        "X_fake = np.random.rand(num_samples, num_features) * 100\n",
        "y_fake = X_fake[:, 0] * 2 + X_fake[:, 1] * 5 + np.random.randn(num_samples) * 10\n",
        "fake_data = np.hstack((X_fake, y_fake.reshape(-1, 1)))\n",
        "\n",
        "df_fake = pd.DataFrame(fake_data, columns=[f'feature_{i+1}' for i in range(num_features)] + ['target'])\n",
        "fake_csv_path = '/tmp/fake_data.csv'\n",
        "df_fake.to_csv(fake_csv_path, index=False)\n",
        "print(f\"Saved fake data to {fake_csv_path}\")\n"
      ],
      "metadata": {
        "id": "zS1xHyWeOIpQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2acd5837-927d-4b41-b446-f43f74e8bcb2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved fake data to /tmp/fake_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model = LinearRegressionScratch(learning_rate=0.01, epochs=500)\n",
        "model.fit(fake_csv_path)\n",
        "\n",
        "# Predict on new data\n",
        "sample_features = [5.1, 3.5, 1.4, 0.2, 2.0]  # Example input\n",
        "print(\"Prediction:\", model.predict(sample_features))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNeGGcV9X8z7",
        "outputId": "b063c784-2ddc-46aa-c305-cac8c386b8c1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50: Loss = 0.7981\n",
            "Epoch 100: Loss = 0.1956\n",
            "Epoch 150: Loss = 0.0533\n",
            "Epoch 200: Loss = 0.0174\n",
            "Epoch 250: Loss = 0.0082\n",
            "Epoch 300: Loss = 0.0058\n",
            "Epoch 350: Loss = 0.0052\n",
            "Epoch 400: Loss = 0.0050\n",
            "Epoch 450: Loss = 0.0050\n",
            "Epoch 500: Loss = 0.0050\n",
            "Prediction: 25.354132\n"
          ]
        }
      ]
    }
  ]
}